# -*- coding: utf-8 -*-
"""GEN_AI_RAG2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iOirYR6B6hPw08nP-H_wvZt9Mmoduyek

## Data Collection
"""

!pip install beautifulsoup4
!pip install requests

"""## Collecting the data"""

import requests
from bs4 import BeautifulSoup
import re

urls = [
    "https://en.wikipedia.org/wiki/Space_exploration",
    "https://en.wikipedia.org/wiki/Apollo_program",
    "https://en.wikipedia.org/wiki/Hubble_Space_Telescope",
    "https://en.wikipedia.org/wiki/Mars_rover",  # Corrected link
    "https://en.wikipedia.org/wiki/International_Space_Station",
    "https://en.wikipedia.org/wiki/SpaceX",
    "https://en.wikipedia.org/wiki/Juno_(spacecraft)",
    "https://en.wikipedia.org/wiki/Voyager_program",
    "https://en.wikipedia.org/wiki/Galileo_(spacecraft)",
    "https://en.wikipedia.org/wiki/Kepler_Space_Telescope",
    "https://en.wikipedia.org/wiki/James_Webb_Space_Telescope",
    "https://en.wikipedia.org/wiki/Space_Shuttle",
    "https://en.wikipedia.org/wiki/Artemis_program",
    "https://en.wikipedia.org/wiki/Skylab",
    "https://en.wikipedia.org/wiki/NASA",
    "https://en.wikipedia.org/wiki/European_Space_Agency",
    "https://en.wikipedia.org/wiki/Ariane_(rocket_family)",
    "https://en.wikipedia.org/wiki/Spitzer_Space_Telescope",
    "https://en.wikipedia.org/wiki/New_Horizons",
    "https://en.wikipedia.org/wiki/Cassini%E2%80%93Huygens",
    "https://en.wikipedia.org/wiki/Curiosity_(rover)",
    "https://en.wikipedia.org/wiki/Perseverance_(rover)",
    "https://en.wikipedia.org/wiki/InSight",
    "https://en.wikipedia.org/wiki/OSIRIS-REx",
    "https://en.wikipedia.org/wiki/Parker_Solar_Probe",
    "https://en.wikipedia.org/wiki/BepiColombo",
    "https://en.wikipedia.org/wiki/Solar_Orbiter",
    "https://en.wikipedia.org/wiki/Gaia_(spacecraft)"
]

"""## Preparing the data"""

#Как исправить
#1. Проверять, что нужный div найден, а HTTP-код ‑ 200.
#2. Добавить User-Agent, чтобы запросы выглядели «браузерными».
#3. Исправить мелкие опечатки в коде (regex и разделители строк).

HEADERS = {'User-Agent': 'Mozilla/5.0'}

def clean_text(text: str) -> str:
    """
    Удаляет ссылочные номера вида [1], [23] и лишние пробелы.
    """
    text = re.sub(r'[d+]', '', text)     # [1] → ''
    return re.sub(r's{2,}', ' ', text)     # сжать повторяющиеся пробелы

def fetch_and_clean(url: str) -> str:
    # 1. получаем страницу
    resp = requests.get(url, headers=HEADERS, timeout=15)
    resp.raise_for_status()                 # поднимет исключение, если код ≠ 200

    soup = BeautifulSoup(resp.content, 'html.parser')

    # 2. основной контент
    content = soup.find('div', class_='mw-parser-output')
    if content is None:                     # если структура неожиданная
        print(f'Не найден основной блок на {url}')
        return ''

    # 3. вырезаем ненужные разделы
    for title in ['References', 'Bibliography', 'External_links', 'See_also']:
        span = content.find('span', id=title)
        if span:
            # удалить всё, что идёт после заголовка
            for sib in list(span.parent.find_next_siblings()):
                sib.decompose()
            span.parent.decompose()

    # 4. текст + очистка
    text = content.get_text(' ', strip=True)
    return clean_text(text)

# запись в файл
with open('llm.txt', 'w', encoding='utf-8') as f:
    for url in urls:
        article = fetch_and_clean(url)
        if article:
            f.write(article + 'n')
print("Content written to llm.txt")

# Open the file and read the first 20 lines
with open('llm.txt', 'r', encoding='utf-8') as file:
    lines = file.readlines()
    # Print the first 20 lines
    for line in lines[:20]:
        print(line.strip())

"""## 2. Embedding-Based Retrieval with Activeloop and OpenAI"""

try:
  import deeplake
except:
  !pip install deeplake
  import deeplake

#GitHub grequests.py
#Script to download files from the GitHub repository.

import subprocess

url = "https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py"
output_file = "grequests.py"

# Prepare the curl command
curl_command = [
    "curl",
    "-o", output_file,
    url
]

# Execute the curl command
try:
    subprocess.run(curl_command, check=True)
    print("Download successful.")
except subprocess.CalledProcessError:
    print("Failed to download the file.")

!pip install -q gigachat

with open('/etc/resolv.conf', 'w') as file:
   file.write("nameserver 8.8.8.8")

import getpass
import textwrap
from gigachat import GigaChat
from gigachat.models import Chat, Messages, MessagesRole

API_KEY = getpass.getpass('Введите ваш GigaChat API Key: ').strip()
client = GigaChat(credentials=API_KEY, verify_ssl_certs=False)

def call_llm_with_full_text(itext):
    prompt_text = "\n".join(itext)
    prompt = f"Please elaborate on the following content:\n{prompt_text}"

    messages = [
        Messages(role=MessagesRole.SYSTEM, content="You are an expert Natural Language Processing exercise expert.You are an expert Natural Language Processing researcher. Always answer in English."),
        Messages(role=MessagesRole.ASSISTANT, content="1.You can read the input and answer in detail"),
        Messages(role=MessagesRole.USER, content=prompt)
    ]

    try:
        response = client.chat(
            Chat(messages=messages, temperature=0.1)
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Ошибка запроса к GigaChat: {e}"

def print_formatted_response(response):
    wrapper = textwrap.TextWrapper(width=80)
    wrapped_text = wrapper.fill(text=response)

    print("Response:")
    print("---------------")
    print(wrapped_text)
    print("---------------\n")

!pip install qdrant-client

from qdrant_client import QdrantClient
client = QdrantClient(":memory:")      # <-- сервер стартует внутри процесса

from sentence_transformers import SentenceTransformer
import numpy as np
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

from grequests import download
source_text = "llm.txt"

directory = "Chapter02"
filename = "llm.txt"
download(directory, filename)

with open(source_text, 'r') as f:
    text = f.read()

CHUNK_SIZE = 1000
chunked_text = [text[i:i+CHUNK_SIZE] for i in range(0,len(text), CHUNK_SIZE)]

vectors = model.encode(chunked_text, normalize_embeddings=True)

from qdrant_client.http.models import (
    VectorParams,
    Distance,
    PointStruct,
)
COLLECTION = "docs"
VECTOR_SIZE = 384

client.recreate_collection(
    collection_name=COLLECTION,
    vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)
)

vectors = vectors.astype("float32")

from qdrant_client.http.models import PointStruct
import uuid

points = [
    PointStruct(
        id=uuid.uuid4().hex,                 # или i, если порядок не важен
        vector=vec.tolist(),                 # .tolist() -> чистый Python-лист
        payload={"text": chunk}              # метаданные
    )
    for vec, chunk in zip(vectors, chunked_text)
]

ds = client.upsert(
    collection_name=COLLECTION,
    points=points
)

hits = client.search(
    collection_name=COLLECTION,
    query_vector=vectors[0],   # любой из ваших векторов
    limit=3
)

for h in hits:
    print(f"id={h.id}, score={h.score:.4f}, text={h.payload['text'][:50]}…")

"""## RAG-retriv (моделирование функций для запроса)"""

from qdrant_client import QdrantClient
from qdrant_client.http import models as qmodels

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models as qmodels

def embedding_function(texts):
    # Принимает str или List[str] и возвращает List[List[float]]
    if isinstance(texts, str):
        texts = [texts]
    texts = [t.replace("n", " ") for t in texts]
    return model.encode(texts, normalize_embeddings=True).tolist()

def search_query(prompt, limit=7):
    qvec = embedding_function(prompt)[0]
    return client.search(collection_name=COLLECTION, query_vector=qvec, limit=limit)

user_prompt = "Tell me about space exploration on the Moon and Mars."
hits = search_query(user_prompt)

for h in hits:
    print(f"id={h.id:<3} score={h.score:.4f} text={h.payload['text'][:60]}…")

def wrap_text(text: str, width: int = 80) -> str:
    """
    Разбивает длинную строку на строки указанной ширины.
    """
    return textwrap.fill(text, width=width)

if not hits:                        # пустой ответ
    print("Ничего не найдено")
else:
    top = hits[0]                   # hits — это список Hit-объектов

    # Если поля разные — поправьте названия
    score   = top.score
    text    = top.payload["text"].strip()
    source  = top.payload.get("source", "unknown")

    print("Top Search Result")
    print(f"Score : {score:.4f}")
    print(f"Source: {source}")
    print("Text:")
    print(wrap_text(text, width=80))

augmented_input=user_prompt+" "+text

print(augmented_input)

"""## Cтруктурирование при помощи GPT (помогает при структурировании запроса)"""

import os
import time
import textwrap
import getpass
from gigachat import GigaChat              # так и остаётся
from gigachat.exceptions import GigaChatException

def call_llm_with_full_text(itext, temperature: float = 0.1) -> str:
    """Отправляет текст в GigaChat и возвращает ответ."""
    prompt_text = "\n".join(itext)
    prompt = f"Please elaborate on the following content:\n{prompt_text}"

    messages = [
        Messages(
            role=MessagesRole.SYSTEM,
            content="You are an expert Natural Language Processing researcher. "
                    "Always answer in English."
        ),
        Messages(
            role=MessagesRole.ASSISTANT,
            content="You can read the input and answer in detail."
        ),
        Messages(role=MessagesRole.USER, content=prompt)
    ]

    try:
        # открываем соединение контекст-менеджером
        with GigaChat(credentials=API_KEY, verify_ssl_certs=False) as client:
            response = client.chat(Chat(messages=messages, temperature=temperature))
            return response.choices[0].message.content.strip()
    except GigaChatException as e:
        return f"GigaChat error: {e}"
    except Exception as e:
        return f"Unexpected error: {e}"

def print_formatted_response(response: str) -> None:
    wrapper = textwrap.TextWrapper(width=80)
    wrapped_text = wrapper.fill(text=response)

    print("Response:")
    print("---------------")
    print(wrapped_text)
    print("---------------\n")

start_time = time.time()

reply = call_llm_with_full_text(augmented_input)
elapsed = time.time() - start_time

print_formatted_response(reply)
print(f"Response time: s")

import textwrap
import re
from IPython.display import display, HTML
import markdown


def print_formatted_response(response: str) -> None:
    """
    Выводит строку как Markdown-HTML (если в ней найдены признаки Markdown)
    или как обычный текст (с переносами строк).

    Parameters
    ----------
    response : str
        Текст, полученный от модели.
    """
    markdown_patterns = [
        r"^#+\s",           # Заголовки (#, ##, ### …)
        r"^\*+\s",          # Списки со звёздочкой
        r"\*\*",            # **bold**
        r"_(?!\s)",         # _italic_  или __bold__  (исключаем одиночный _ )
        r"\[.+?\]\(.+?\)",  # [link](url)
        r"^\-\s",           # Списки с дефисом
        r"```"              # Блоки кода
    ]
    is_markdown = any(
        re.search(pattern, response, re.MULTILINE) for pattern in markdown_patterns
    )

    if is_markdown:
        html_output = markdown.markdown(
            response,
            extensions=[
                "fenced_code",     # поддержка ```code```
                "tables",          # markdown-таблицы
                "sane_lists"       # корректная работа списков
            ]
        )
        display(HTML(html_output))
    else:
        wrapper = textwrap.TextWrapper(width=80)
        wrapped_text = wrapper.fill(text=response)
        print("Text Response:")
        print(wrapped_text)

print_formatted_response(reply)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_similarity(text1, text2):
    vectorizer = TfidfVectorizer()
    tfidf = vectorizer.fit_transform([text1, text2])
    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
    return similarity[0][0]

similarity_score = calculate_cosine_similarity(user_prompt, reply)

print(f"Cosine Similarity Score: {similarity_score:.3f}")

similarity_score = calculate_cosine_similarity(augmented_input, reply) ## нормированный текст (ввод пользователя + текст - только верхний результат)

print(f"Cosine Similarity Score: {similarity_score:.3f}")

def calculate_cosine_similarity_with_embeddings(text1, text2):
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)
    similarity = cosine_similarity([embeddings1], [embeddings2])
    return similarity[0][0]


similarity_score = calculate_cosine_similarity_with_embeddings(augmented_input, reply)
print(f"Cosine Similarity Score: {similarity_score:.3f}")